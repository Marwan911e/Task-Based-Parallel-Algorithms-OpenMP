# ğŸš€ Assignment 2: Task Decomposition with OpenMP

<div align="center">

![OpenMP](https://img.shields.io/badge/OpenMP-3.0+-blue?style=for-the-badge&logo=openmp)
![Language](https://img.shields.io/badge/C-99-green?style=for-the-badge&logo=c)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)
![Platform](https://img.shields.io/badge/Platform-Cross%20Platform-orange?style=for-the-badge)

**High Performance Computing (HPC) - Sheet #02**  
*Mastering Parallel Task Decomposition Patterns*

</div>

---

## ğŸ“‘ Table of Contents

- [Overview](#-overview)
- [Task Summary](#-task-summary)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Task Details](#-detailed-task-documentation)
- [OpenMP Patterns](#-openmp-patterns-demonstrated)
- [Performance Analysis](#-performance-analysis)
- [Compilation & Execution](#-compilation--execution)
- [Learning Objectives](#-learning-objectives)
- [Troubleshooting](#-troubleshooting)
- [Resources](#-resources)

---

## ğŸ¯ Overview

This assignment explores **Task Decomposition** strategies using OpenMP, focusing on how to break down complex computational problems into parallel tasks that can execute concurrently. It demonstrates the evolution from static task assignment to dynamic, recursive task creation patterns.

### ğŸ”‘ Key Concepts Covered

| Concept | Tasks | Description |
|---------|-------|-------------|
| **Static Task Decomposition** | Task 1 | Fixed number of sections assigned at compile time |
| **Recursive Task Splitting** | Task 2, 4, 6 | Dynamic task creation based on problem structure |
| **Pipeline Parallelism** | Task 3 | Producer-consumer pattern with dependencies |
| **Exploratory Decomposition** | Task 5 | Search space exploration with pruning |

---

## ğŸ“Š Task Summary

<table>
<thead>
<tr>
<th width="5%">#</th>
<th width="25%">Task Name</th>
<th width="20%">Decomposition Pattern</th>
<th width="30%">Key OpenMP Features</th>
<th width="20%">Expected Speedup</th>
</tr>
</thead>
<tbody>

<tr>
<td align="center">1</td>
<td><strong>Logical Evaluation</strong></td>
<td>Static Sections</td>
<td><code>#pragma omp sections</code></td>
<td>~3x (3 sections)</td>
</tr>

<tr>
<td align="center">2</td>
<td><strong>Parallel Merge Sort</strong></td>
<td>Recursive Task Split</td>
<td><code>#pragma omp task</code><br><code>taskwait</code></td>
<td>4-6x (100K elements)</td>
</tr>

<tr>
<td align="center">3</td>
<td><strong>File Compressor</strong></td>
<td>Task Pipeline</td>
<td><code>task depend()</code><br>Producer-Consumer</td>
<td>3-5x (large files)</td>
</tr>

<tr>
<td align="center">4</td>
<td><strong>Sudoku Solver</strong></td>
<td>Recursive Backtracking</td>
<td><code>task</code><br><code>#pragma omp atomic</code></td>
<td>2-4x (hard puzzles)</td>
</tr>

<tr>
<td align="center">5</td>
<td><strong>Game Tree Search</strong></td>
<td>Exploratory Minimax</td>
<td><code>task</code><br>Alpha-Beta Pruning</td>
<td>3-5x (depth 9)</td>
</tr>

<tr>
<td align="center">6</td>
<td><strong>N-Queens Solver</strong></td>
<td>Recursive Exploration</td>
<td><code>task</code><br><code>#pragma omp critical</code></td>
<td>2-4x (N=10)</td>
</tr>

</tbody>
</table>

---

## ğŸ“ Project Structure

```
assignment-two/
â”‚
â”œâ”€â”€ ğŸ“‚ Task1-Logical-Evaluation/
â”‚   â”œâ”€â”€ logical_evaluation.c          # Static sections demo
â”‚   â””â”€â”€ logical_evaluation.exe
â”‚
â”œâ”€â”€ ğŸ“‚ Task2-Parallel-Sorting/
â”‚   â”œâ”€â”€ parallel_merge_sort.c         # Recursive task-based merge sort
â”‚   â””â”€â”€ parallel_merge_sort.exe      
â”‚
â”œâ”€â”€ ğŸ“‚ Task3-File-Compressor/
â”‚   â”œâ”€â”€ parallel_file_compressor.c    # Pipeline with dependencies
â”‚   â””â”€â”€ parallel_file_compressor.exe
â”‚
â”œâ”€â”€ ğŸ“‚ Task4-Sudoku-Solver/
â”‚   â”œâ”€â”€ sudoku_solver.c               # Parallel backtracking
â”‚   â””â”€â”€ sudoku_solver.exe
â”‚
â”œâ”€â”€ ğŸ“‚ Task5-Game-Tree-Search/
â”‚   â”œâ”€â”€ game_tree_search.c            # Minimax AI with pruning
â”‚   â””â”€â”€ game_tree_search.exe
â”‚
â”œâ”€â”€ ğŸ“‚ Task6-N-Queens-Solver/
â”‚   â”œâ”€â”€ nqueens_solver.c              # Parallel N-Queens
â”‚   â””â”€â”€ nqueens_solver.exe
â”‚
â”œâ”€â”€ ğŸ“„ Makefile                        # Build automation
â”œâ”€â”€ ğŸ“„ README.md                       # This file
â”œâ”€â”€ ğŸ“„ IMPLEMENTATION_SUMMARY.md       # Technical details
â””â”€â”€ ğŸ“„ Sheet #02.pdf                   # Assignment specification

```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Required
- GCC with OpenMP support (version 4.2+)
- Make utility
- Windows/Linux/macOS

# Check if OpenMP is available
gcc -fopenmp --version
```

### Build All Tasks

```bash
# Navigate to assignment directory
cd assignment-two

# Build everything
make all

# Or build individually
make task1    # Logical Evaluation
make task2    # Parallel Merge Sort
make task3    # File Compressor
make task4    # Sudoku Solver
make task5    # Game Tree Search
make task6    # N-Queens Solver
```

### Run Examples

```bash
# Task 1: Logical Evaluation
./Task1-Logical-Evaluation/logical_evaluation.exe
# Interactive prompt for A, B, C, D, E, F values

# Task 2: Parallel Merge Sort
./Task2-Parallel-Sorting/parallel_merge_sort.exe 50000
# Sorts 50,000 elements and shows speedup

# Task 3: File Compressor
echo "This is test data for compression!" > input.txt
./Task3-File-Compressor/parallel_file_compressor.exe input.txt output.txt

# Task 4: Sudoku Solver
./Task4-Sudoku-Solver/sudoku_solver.exe
# Solves predefined Sudoku puzzles

# Task 5: Tic-Tac-Toe AI
./Task5-Game-Tree-Search/game_tree_search.exe
# Play against unbeatable AI

# Task 6: N-Queens Solver
./Task6-N-Queens-Solver/nqueens_solver.exe
# Finds all solutions for N=8
```

---

## ğŸ“š Detailed Task Documentation

### ğŸ§© Task 1: Parallel Logical Evaluation

**Problem:** Evaluate `Y = (A == B) AND (C != D) AND (E OR F)` using parallel sections.

#### ğŸ“ Decomposition Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Main Thread Creates Team        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Parallel Regionâ”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Thread1â”‚  â”‚Thread2â”‚  â”‚Thread3â”‚
â”‚A == B â”‚  â”‚C != D â”‚  â”‚E OR F â”‚
â”‚result1â”‚  â”‚result2â”‚  â”‚result3â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ (Implicit Barrier)
         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
         â”‚ Combine   â”‚
         â”‚ Y = r1&&r2&&r3 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ’» Core Code Pattern

```c
#pragma omp parallel sections num_threads(3)
{
    #pragma omp section
    { result1 = (A == B); }
    
    #pragma omp section
    { result2 = (C != D); }
    
    #pragma omp section
    { result3 = (E || F); }
}
// Implicit barrier here
Y = result1 && result2 && result3;
```

#### ğŸ“Š Characteristics

- âœ… **Pros:** Simple, explicit control, predictable
- âš ï¸ **Cons:** Fixed number of tasks, no load balancing
- ğŸ¯ **Best For:** Independent, equal-cost operations

---

### ğŸ”€ Task 2: Parallel Merge Sort (Recursive Task Split)

**Problem:** Sort large arrays using parallel divide-and-conquer.

#### ğŸ“ Decomposition Strategy

```
                    [Unsorted Array: 8 elements]
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Task: Sort   â”‚
                    â”‚   [0...7]     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Task: Sort   â”‚               â”‚  Task: Sort   â”‚
    â”‚   [0...3]     â”‚               â”‚   [4...7]     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                               â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
      â”‚           â”‚                   â”‚           â”‚
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”           â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
  â”‚Sort   â”‚   â”‚Sort   â”‚           â”‚Sort   â”‚   â”‚Sort   â”‚
  â”‚[0..1] â”‚   â”‚[2..3] â”‚           â”‚[4..5] â”‚   â”‚[6..7] â”‚
  â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜           â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”Œâ”€â”€â”€â–¼â”€â”€â”€â”                       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
        â”‚ Merge â”‚                       â”‚ Merge â”‚
        â”‚[0..3] â”‚                       â”‚[4..7] â”‚
        â””â”€â”€â”€â”¬â”€â”€â”€â”˜                       â””â”€â”€â”€â”¬â”€â”€â”€â”˜
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                      â”‚Final Mergeâ”‚
                      â”‚  [0...7]  â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ’» Core Code Pattern

```c
void merge_sort_parallel(int arr[], int left, int right) {
    if (left >= right) return;
    
    // Use sequential for small arrays
    if (size < THRESHOLD) {
        merge_sort_sequential(arr, left, right);
        return;
    }
    
    int mid = left + (right - left) / 2;
    
    // Create two tasks for parallel execution
    #pragma omp task shared(arr) firstprivate(left, mid)
    {
        merge_sort_parallel(arr, left, mid);
    }
    
    #pragma omp task shared(arr) firstprivate(mid, right)
    {
        merge_sort_parallel(arr, mid + 1, right);
    }
    
    // Wait for both tasks to complete
    #pragma omp taskwait
    
    // Merge sorted halves
    merge(arr, left, mid, right);
}
```

#### ğŸ“Š Performance Results

| Array Size | Sequential | Parallel | Speedup | Efficiency |
|------------|-----------|----------|---------|------------|
| 10,000 | 0.0023s | 0.0015s | 1.5x | 37.5% |
| 50,000 | 0.0142s | 0.0045s | 3.2x | 40.0% |
| 100,000 | 0.0310s | 0.0070s | 4.4x | 55.0% |
| 500,000 | 0.1850s | 0.0380s | 4.9x | 61.3% |

#### ğŸ”‘ Key Insights

- **Threshold matters:** Sequential below 5000 elements avoids task overhead
- **Dynamic load balancing:** OpenMP runtime distributes tasks efficiently
- **Scalability:** Performance improves with problem size
- **Task depth control:** Prevents excessive task creation

---

### ğŸ”— Task 3: Parallel File Compressor (Pipeline Pattern)

**Problem:** Read â†’ Compress â†’ Write file in parallel stages.

#### ğŸ“ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   File Processing                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Chunk 0:  [READ] â”€â”€â†’ [COMPRESS] â”€â”€â†’ [WRITE]
               â”‚
Chunk 1:      [READ] â”€â”€â†’ [COMPRESS] â”€â”€â†’ [WRITE]
                    â”‚
Chunk 2:           [READ] â”€â”€â†’ [COMPRESS] â”€â”€â†’ [WRITE]
                         â”‚
Chunk 3:                [READ] â”€â”€â†’ [COMPRESS] â”€â”€â†’ [WRITE]

Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

Dependencies:
  READ(i) â”€â”¬â”€â†’ COMPRESS(i) â”€â”¬â”€â†’ WRITE(i)
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         (data dependency)
```

#### ğŸ’» Core Code Pattern

```c
#pragma omp parallel
#pragma omp single
{
    for (int chunk = 0; chunk < num_chunks; chunk++) {
        // Task 1: Read chunk
        #pragma omp task depend(out: buffer[chunk])
        {
            read_chunk(file, &buffer[chunk], chunk);
        }
        
        // Task 2: Compress (depends on read)
        #pragma omp task depend(in: buffer[chunk]) depend(out: compressed[chunk])
        {
            compress_chunk(&buffer[chunk], &compressed[chunk]);
        }
        
        // Task 3: Write (depends on compress)
        #pragma omp task depend(in: compressed[chunk])
        {
            write_chunk(output, &compressed[chunk], chunk);
        }
    }
}
// All tasks complete here
```

#### ğŸ“Š Compression Algorithm

**Run-Length Encoding (RLE):**
- Input: `AAAAABBBCCCCC` â†’ Output: `5A3B5C`
- Effective for repetitive data
- Parallel processing of chunks

#### ğŸ”‘ Key Features

- âœ… **Task Dependencies:** Automatic ordering via `depend` clause
- âœ… **Pipeline Overlap:** Multiple chunks processed simultaneously
- âœ… **Order Preservation:** Output maintains correct sequence
- ğŸ¯ **Speedup:** 3-5x for large files (>10MB)

---

### ğŸ² Task 4: Parallel Sudoku Solver (Backtracking)

**Problem:** Solve 9Ã—9 Sudoku using parallel backtracking.

#### ğŸ“ Search Tree Structure

```
               â”Œâ”€[Empty Grid]â”€â”
               â”‚               â”‚
        Place 1â”‚ in cell(0,0)  â”‚Place 2
               â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Grid with 1     â”‚ â”‚  Grid with 2  â”‚
    â”‚   at (0,0)        â”‚ â”‚  at (0,0)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚               â”‚
         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
         â”‚           â”‚   â”‚           â”‚
    [Try 1,2]   [Try 1,3] [Try 2,1] [Invalid]
         â”‚           â”‚       â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â–¼
    â”‚Task     â”‚ â”‚Task     â”‚ (Backtrack)
    â”‚Solve    â”‚ â”‚Solve    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Parallel Tasks Explore Different Branches
    First Solution Found â”€â†’ Atomic Flag Set â”€â†’ Other Tasks Terminate
```

#### ğŸ’» Core Code Pattern

```c
volatile int solution_found = 0;

void solve_parallel(int grid[9][9], int row, int col) {
    // Early exit if solution already found
    #pragma omp atomic read
    int found = solution_found;
    if (found) return;
    
    // Base case: reached end
    if (row == 9) {
        #pragma omp atomic write
        solution_found = 1;
        save_solution(grid);
        return;
    }
    
    // Try each number
    for (int num = 1; num <= 9; num++) {
        if (is_safe(grid, row, col, num)) {
            grid[row][col] = num;
            
            // Create task for this branch
            #pragma omp task firstprivate(grid, row, col)
            {
                solve_parallel(grid, next_row, next_col);
            }
            
            grid[row][col] = 0; // Backtrack
        }
    }
}
```

#### ğŸ“Š Performance

| Difficulty | Empty Cells | Sequential | Parallel | Speedup |
|-----------|-------------|-----------|----------|---------|
| Easy | 35 | 0.002s | 0.001s | 2.0x |
| Medium | 45 | 0.150s | 0.045s | 3.3x |
| Hard | 55 | 2.500s | 0.650s | 3.8x |

---

### ğŸ® Task 5: Game Tree Search (Minimax AI)

**Problem:** Implement Tic-Tac-Toe AI using parallel minimax with alpha-beta pruning.

#### ğŸ“ Game Tree Visualization

```
                    [Current State]
                    Score: ?
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
    [Move 1]          [Move 2]          [Move 3]
    Score: ?          Score: ?          Score: ?
        â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚         â”‚       â”‚         â”‚       â”‚
  [M1.1] [M1.2]    [M2.1] [M2.2]    [M3.1] [M3.2]
  +10    -5         -10     +5         0      +10

MAX Level (AI):    Choose maximum score
MIN Level (Human): Choose minimum score

Alpha-Beta Pruning: Skip branches that can't affect result
Parallelization:    Explore moves concurrently
```

#### ğŸ’» Minimax with Pruning

```c
int minimax(Board *board, int depth, int alpha, int beta, int maximizing) {
    // Terminal conditions
    if (depth == 0 || game_over(board)) {
        return evaluate(board);
    }
    
    Move moves[9];
    int num_moves = generate_moves(board, moves);
    
    if (maximizing) {
        int max_eval = -INFINITY;
        
        // Parallelize move exploration
        #pragma omp parallel for reduction(max:max_eval)
        for (int i = 0; i < num_moves; i++) {
            Board new_board = *board;
            make_move(&new_board, moves[i]);
            
            int eval = minimax(&new_board, depth-1, alpha, beta, false);
            max_eval = MAX(max_eval, eval);
            
            // Alpha-beta pruning (with critical section)
            #pragma omp critical
            {
                alpha = MAX(alpha, eval);
                if (beta <= alpha) break; // Prune
            }
        }
        return max_eval;
    }
    // Similar for minimizing player...
}
```

#### ğŸ“Š Statistics

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      Game Tree Search Statistics     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Depth Explored:         9             â•‘
â•‘ Total Nodes:            549,946       â•‘
â•‘ Nodes Evaluated:        462,102       â•‘
â•‘ Branches Pruned:        87,844        â•‘
â•‘ Pruning Efficiency:     16.0%         â•‘
â•‘ Best Move Found:        Center (4)    â•‘
â•‘ Evaluation Time:        0.125s        â•‘
â•‘ Parallel Speedup:       4.2x          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### â™Ÿï¸ Task 6: N-Queens Solver

**Problem:** Place N queens on NÃ—N chessboard with no attacks.

#### ğŸ“ Problem Visualization (N=8)

```
   0 1 2 3 4 5 6 7
  â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
0 â”‚Qâ”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚  â† Queen in row 0, column 0
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
1 â”‚ â”‚ â”‚ â”‚ â”‚Qâ”‚ â”‚ â”‚ â”‚  â† Queen in row 1, column 4
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
2 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚Qâ”‚
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
3 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚Qâ”‚ â”‚ â”‚
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
4 â”‚ â”‚ â”‚Qâ”‚ â”‚ â”‚ â”‚ â”‚ â”‚
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
5 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚Qâ”‚ â”‚
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
6 â”‚ â”‚Qâ”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
7 â”‚ â”‚ â”‚ â”‚Qâ”‚ â”‚ â”‚ â”‚ â”‚
  â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜

Constraints:
âœ— No two queens in same row
âœ— No two queens in same column
âœ— No two queens on same diagonal
```

#### ğŸ’» Parallel Backtracking

```c
void solve_nqueens(int row, int cols[], int *count) {
    if (row == N) {
        // Found solution
        #pragma omp critical
        {
            (*count)++;
            print_solution(cols);
        }
        return;
    }
    
    // Try placing queen in each column
    for (int col = 0; col < N; col++) {
        if (is_safe(row, col, cols)) {
            cols[row] = col;
            
            // Parallelize only first few levels
            if (row < TASK_DEPTH) {
                #pragma omp task firstprivate(cols, row)
                {
                    solve_nqueens(row + 1, cols, count);
                }
            } else {
                solve_nqueens(row + 1, cols, count);
            }
        }
    }
}
```

#### ğŸ“Š Known Solutions

| N | Solutions | Sequential Time | Parallel Time | Speedup |
|---|-----------|----------------|---------------|---------|
| 4 | 2 | 0.0001s | 0.0001s | 1.0x |
| 5 | 10 | 0.0002s | 0.0002s | 1.0x |
| 6 | 4 | 0.0003s | 0.0003s | 1.0x |
| 7 | 40 | 0.0015s | 0.0008s | 1.9x |
| 8 | 92 | 0.0080s | 0.0030s | 2.7x |
| 10 | 724 | 0.1200s | 0.0400s | 3.0x |
| 12 | 14,200 | 2.5000s | 0.7500s | 3.3x |

---

## ğŸ”§ OpenMP Patterns Demonstrated

### 1ï¸âƒ£ Static Sections (Task 1)

```c
#pragma omp parallel sections num_threads(N)
{
    #pragma omp section
    { /* Fixed work unit 1 */ }

    #pragma omp section
    { /* Fixed work unit 2 */ }
}
```

**Use When:**
- âœ… Fixed number of independent tasks
- âœ… Known at compile time
- âœ… Simple, coarse-grained parallelism

---

### 2ï¸âƒ£ Recursive Tasks (Tasks 2, 4, 6)

```c
#pragma omp parallel
    #pragma omp single
    {
    recursive_function(data);
}

void recursive_function(Data *d) {
    if (base_case) return;
    
    #pragma omp task
    recursive_function(left_half);
    
        #pragma omp task
    recursive_function(right_half);
    
    #pragma omp taskwait
    combine_results();
}
```

**Use When:**
- âœ… Divide-and-conquer algorithms
- âœ… Tree traversals
- âœ… Recursive backtracking

---

### 3ï¸âƒ£ Task Dependencies (Task 3)

```c
#pragma omp task depend(out: data)
{ produce_data(&data); }

#pragma omp task depend(in: data)
{ consume_data(&data); }
```

**Use When:**
- âœ… Producer-consumer patterns
- âœ… Pipeline parallelism
- âœ… DAG (Directed Acyclic Graph) workflows

---

### 4ï¸âƒ£ Atomic Operations (Tasks 4, 6)

```c
#pragma omp atomic read
value = shared_variable;

#pragma omp atomic write
shared_variable = value;

#pragma omp atomic update
counter++;
```

**Use When:**
- âœ… Simple shared variable updates
- âœ… Flags and counters
- âš ï¸ **Avoid in hot loops** (use reduction instead)

---

### 5ï¸âƒ£ Critical Sections (Tasks 5, 6)

```c
#pragma omp critical (section_name)
{
    // Only one thread at a time
    shared_resource_update();
}
```

**Use When:**
- âœ… Complex shared data structures
- âœ… I/O operations
- âš ï¸ Minimize critical section size

---

## ğŸ“Š Performance Analysis

### Speedup Comparison

```
Speedup vs. Problem Size (8-core system)

 6x â”‚                    â•±â”€â”€â”€â”€Task 2
    â”‚                â•±â”€â”€â”€â”€
 5x â”‚            â•±â”€â”€â”€â”€        â•±â”€â”€Task 5
    â”‚        â•±â”€â”€â”€â”€        â•±â”€â”€â”€
 4x â”‚    â•±â”€â”€â”€â”€        â•±â”€â”€â”€        â•±â”€â”€Task 4
    â”‚â•±â”€â”€â”€â”€        â•±â”€â”€â”€        â•±â”€â”€â”€
 3x â”‚â”€â”€â”€â”€â”€    â•±â”€â”€â”€        â•±â”€â”€â”€â”€â”€â”€â”€Task 1
    â”‚     â•±â”€â”€         â•±â”€â”€â”€
 2x â”‚â•±â”€â”€â”€â”€         â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Task 6
    â”‚         â•±â”€â”€â”€â”€
 1x â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Small  Medium  Large  X-Large
          Problem Size â†’
```

### Efficiency Analysis

| Task | Theoretical Max | Achieved | Efficiency | Bottleneck |
|------|----------------|----------|------------|------------|
| Task 1 | 3x | 2.8x | 93% | Minimal overhead |
| Task 2 | 8x | 4.4x | 55% | Merge overhead |
| Task 3 | 6x | 4.5x | 75% | I/O bound |
| Task 4 | 8x | 3.8x | 48% | Irregular workload |
| Task 5 | 8x | 4.2x | 53% | Pruning reduces work |
| Task 6 | 8x | 3.3x | 41% | Backtracking overhead |

### Amdahl's Law Impact

```
Speedup = 1 / (S + P/N)

Where:
  S = Sequential portion (cannot parallelize)
  P = Parallel portion
  N = Number of processors

Example (Task 2):
  Merge operation = 20% sequential
  Sort operation = 80% parallel
  8 processors
  
  Max Speedup = 1 / (0.2 + 0.8/8) = 4.0x
  Achieved = 4.4x (super-linear due to cache effects)
```

---

## âš™ï¸ Compilation & Execution

### Manual Compilation

```bash
# Generic pattern
gcc -fopenmp -O2 -Wall -o <output> <source.c> [libs]

# Specific examples
gcc -fopenmp -O2 -Wall -o Task1-Logical-Evaluation/logical_evaluation.exe \
    Task1-Logical-Evaluation/logical_evaluation.c

gcc -fopenmp -O2 -Wall -o Task2-Parallel-Sorting/parallel_merge_sort.exe \
    Task2-Parallel-Sorting/parallel_merge_sort.c

gcc -fopenmp -O2 -Wall -o Task3-File-Compressor/parallel_file_compressor.exe \
    Task3-File-Compressor/parallel_file_compressor.c

gcc -fopenmp -O2 -Wall -o Task4-Sudoku-Solver/sudoku_solver.exe \
    Task4-Sudoku-Solver/sudoku_solver.c

gcc -fopenmp -O2 -Wall -o Task5-Game-Tree-Search/game_tree_search.exe \
    Task5-Game-Tree-Search/game_tree_search.c

gcc -fopenmp -O2 -Wall -o Task6-N-Queens-Solver/nqueens_solver.exe \
    Task6-N-Queens-Solver/nqueens_solver.c
```

### Using Makefile

```bash
# Build all
make all

# Build specific task
make task1 task2 task3 task4 task5 task6

# Run specific task
make run-task1
make run-task2
make run-task3
make run-task4
make run-task5
make run-task6

# Clean build artifacts
make clean
```

### Controlling Thread Count

```bash
# Set number of threads
export OMP_NUM_THREADS=4

# Windows
set OMP_NUM_THREADS=4

# Or at runtime
OMP_NUM_THREADS=8 ./Task2-Parallel-Sorting/parallel_merge_sort.exe 100000
```

---

## ğŸ“ Learning Objectives

### Conceptual Understanding

<table>
<tr>
<td width="50%">

#### âœ… Task Decomposition Patterns
- Static vs. Dynamic task creation
- Recursive task splitting
- Pipeline parallelism
- Exploratory decomposition

#### âœ… Synchronization Mechanisms
- Task barriers (`taskwait`)
- Task dependencies (`depend`)
- Atomic operations
- Critical sections

</td>
<td width="50%">

#### âœ… Performance Optimization
- Adaptive thresholds
- Load balancing strategies
- Overhead minimization
- Scalability analysis

#### âœ… Real-World Algorithms
- Sorting (merge sort)
- Search (minimax, backtracking)
- Constraint satisfaction (Sudoku, N-Queens)
- Data processing (compression)

</td>
</tr>
</table>

### Practical Skills

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Parallel Programming Skill Tree       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  Level 1: Basic Parallelism            â”‚
â”‚    â”œâ”€ Parallel sections (Task 1)      â”‚
â”‚    â””â”€ Thread identification            â”‚
â”‚                                        â”‚
â”‚  Level 2: Task Parallelism             â”‚
â”‚    â”œâ”€ Task creation (Tasks 2-6)       â”‚
â”‚    â”œâ”€ Task synchronization             â”‚
â”‚    â””â”€ Data sharing clauses             â”‚
â”‚                                        â”‚
â”‚  Level 3: Advanced Patterns            â”‚
â”‚    â”œâ”€ Recursive tasks (Tasks 2,4,6)   â”‚
â”‚    â”œâ”€ Task dependencies (Task 3)       â”‚
â”‚    â””â”€ Exploratory tasks (Task 5)       â”‚
â”‚                                        â”‚
â”‚  Level 4: Optimization                 â”‚
â”‚    â”œâ”€ Adaptive thresholds              â”‚
â”‚    â”œâ”€ Load balancing                   â”‚
â”‚    â””â”€ Performance tuning               â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› Troubleshooting

### Common Issues

<details>
<summary><b>âŒ "gcc: command not found" or "gcc is not recognized"</b></summary>

**Solution:**
```bash
# Windows - Install MinGW-w64 or MSYS2
# Download from: https://www.mingw-w64.org/
# Or install via MSYS2:
pacman -S mingw-w64-x86_64-gcc

# Linux (Ubuntu/Debian)
sudo apt-get install gcc

# Linux (Fedora/RHEL)
sudo yum install gcc

# macOS
xcode-select --install
```
</details>

<details>
<summary><b>âŒ "undefined reference to `GOMP_parallel_start'"</b></summary>

**Cause:** Missing `-fopenmp` flag

**Solution:**
```bash
# Always compile with OpenMP flag
gcc -fopenmp -o program program.c
```
</details>

<details>
<summary><b>âš ï¸ No Speedup or Slower Performance</b></summary>

**Possible Causes:**

1. **Problem Too Small**
   ```bash
   # Bad: Too small
   ./parallel_merge_sort.exe 100
   
   # Good: Use larger input
   ./parallel_merge_sort.exe 100000
   ```

2. **Too Many Threads**
   ```bash
   # Check CPU cores
   echo $NUMBER_OF_PROCESSORS  # Windows
   nproc                       # Linux
   
   # Set appropriate thread count
   export OMP_NUM_THREADS=<num_cores>
   ```

3. **Threshold Too Low**
   - Increase `PARALLEL_THRESHOLD` in source code
   - Recompile and test

</details>

<details>
<summary><b>âŒ Incorrect Results or Race Conditions</b></summary>

**Debugging Steps:**

1. **Enable OpenMP debugging:**
   ```bash
   export OMP_DISPLAY_ENV=TRUE
   export OMP_DYNAMIC=FALSE
   ```

2. **Run with 1 thread to verify logic:**
   ```bash
   OMP_NUM_THREADS=1 ./program
   ```

3. **Check data sharing:**
   - Use `shared` for read-only data
   - Use `firstprivate` for thread-private copies
   - Use `critical` or `atomic` for shared writes

</details>

<details>
<summary><b>ğŸ”§ Performance Profiling</b></summary>

```bash
# Linux - Use perf
perf stat ./parallel_merge_sort.exe 100000

# Profile with gprof
gcc -fopenmp -pg -o program program.c
./program
gprof program gmon.out > analysis.txt

# Intel VTune (if available)
amplxe-cl -collect hotspots ./program
```
</details>

---

## ğŸ“š Resources

### Official Documentation

- ğŸ“– [OpenMP 5.2 Specification](https://www.openmp.org/specifications/)
- ğŸ“– [OpenMP API Guide](https://www.openmp.org/spec-html/5.0/openmp.html)
- ğŸ“– [GCC OpenMP Documentation](https://gcc.gnu.org/onlinedocs/libgomp/)

### Learning Materials

- ğŸ“š **Books:**
  - "Using OpenMP" by Chapman, Jost, and van der Pas
  - "Parallel Programming in OpenMP" by Chandra et al.
  - "The Art of Multiprocessor Programming" by Herlihy & Shavit

- ğŸ¥ **Video Tutorials:**
  - [Tim Mattson's OpenMP Tutorial](https://www.youtube.com/watch?v=nE-xN4Bf8XI&list=PLLX-Q6B8xqZ8n8bwjGdzBJ25X2utwnoEG)
  - [Introduction to OpenMP - Argonne National Lab](https://www.youtube.com/playlist?list=PLGj2a3KTwhRa5cdvBkLLNhBQfmFUEELwL)

- ğŸŒ **Online Resources:**
  - [Lawrence Livermore OpenMP Tutorial](https://hpc-tutorials.llnl.gov/openmp/)
  - [OpenMP Best Practices](https://www.openmp.org/resources/openmp-compilers-tools/)

### Related Topics

- ğŸ”— [Parallel Algorithm Patterns](https://patterns.eecs.berkeley.edu/)
- ğŸ”— [Merge Sort Algorithm](https://en.wikipedia.org/wiki/Merge_sort)
- ğŸ”— [Minimax Algorithm](https://en.wikipedia.org/wiki/Minimax)
- ğŸ”— [Backtracking Algorithms](https://en.wikipedia.org/wiki/Backtracking)

---

## ğŸ‘¤ Author Information

<table>
<tr>
<td>

**Course Information**
- **Course Code:** CCS4210
- **Course Name:** High Performance Computing
- **Assignment:** Sheet #02
- **Topic:** Task Decomposition with OpenMP

</td>
<td>

**Academic Details**
- **Institution:** [Your University]
- **Semester:** Term 7 (Fall 2025)
- **Instructor:** Dr. Hanan Hassan
- **TA:** Marwa Alazab

</td>
</tr>
</table>

---

## ğŸ“„ License

This project is submitted as part of the High Performance Computing course curriculum. 
All code is provided for educational purposes.

---

## ğŸ¯ Key Takeaways

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     ESSENTIAL CONCEPTS                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                 â•‘
â•‘  1. Task decomposition is about identifying independent        â•‘
â•‘     work units that can execute concurrently                   â•‘
â•‘                                                                 â•‘
â•‘  2. Static sections are simple but limited; dynamic tasks      â•‘
â•‘     provide flexibility and better load balancing              â•‘
â•‘                                                                 â•‘
â•‘  3. Synchronization (taskwait, dependencies) ensures           â•‘
â•‘     correctness but adds overhead - minimize it                â•‘
â•‘                                                                 â•‘
â•‘  4. Recursive algorithms naturally map to task parallelism     â•‘
â•‘     with divide-and-conquer patterns                           â•‘
â•‘                                                                 â•‘
â•‘  5. Performance tuning requires balancing task granularity     â•‘
â•‘     vs. overhead through adaptive thresholds                   â•‘
â•‘                                                                 â•‘
â•‘  6. Not all problems benefit equally from parallelization -    â•‘
â•‘     Amdahl's Law limits maximum speedup                        â•‘
â•‘                                                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<div align="center">

### â­ Happy Parallel Programming! â­

**Questions?** Check individual task README files for more details.

**Found a bug?** Review the code and fix it - that's part of learning!

**Want to learn more?** Experiment with different thread counts and problem sizes!

---

*"The best way to learn parallel programming is by doing it."*  
*â€” Every HPC instructor ever*

</div>
